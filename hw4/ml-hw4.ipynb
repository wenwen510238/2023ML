{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"zC5KwRyl6Flp"},"source":["# Task description\n","- Classify the speakers of given features.\n","- Main goal: Learn how to use transformer.\n","- Baselines:\n","  - Easy: Run sample code and know how to use transformer.\n","  - Medium: Know how to adjust parameters of transformer.\n","  - Hard: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer. \n","\n","- Other links\n","  - Kaggle: [link](https://www.kaggle.com/t/859c9ca9ede14fdea841be627c412322)\n","  - Slide: [link](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW04/HW04.pdf)\n","  - Data: [link](https://drive.google.com/file/d/1T0RPnu-Sg5eIPwQPfYysipfcz81MnsYe/view?usp=sharing)\n","  - Video (Chinese): [link](https://www.youtube.com/watch?v=EPerg2UnGaI)\n","  - Video (English): [link](https://www.youtube.com/watch?v=Gpz6AUvCak0)\n","  - Solution for downloading dataset fail.: [link](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e?usp=sharing)"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T18:15:33.097254Z","iopub.status.busy":"2023-05-01T18:15:33.096838Z","iopub.status.idle":"2023-05-01T18:15:33.102492Z","shell.execute_reply":"2023-05-01T18:15:33.101340Z","shell.execute_reply.started":"2023-05-01T18:15:33.097181Z"},"trusted":true},"outputs":[],"source":["# pip install conformer"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TPDoreyypeJE"},"source":["# Download dataset\n","- Please follow [here](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e?usp=sharing) to download data\n","- Data is [here](https://drive.google.com/file/d/1FCjdQkMk9YAIhRkUjbbbv89YO9HhPD8J/view?usp=sharing)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T18:15:33.105645Z","iopub.status.busy":"2023-05-01T18:15:33.105197Z","iopub.status.idle":"2023-05-01T18:15:33.114145Z","shell.execute_reply":"2023-05-01T18:15:33.113070Z","shell.execute_reply.started":"2023-05-01T18:15:33.105608Z"},"id":"QvpaILXnJIcw","outputId":"077ee1d4-0c2f-4897-d36e-6a95bd989173","trusted":true},"outputs":[],"source":["# !gdown --id '1FCjdQkMk9YAIhRkUjbbbv89YO9HhPD8J' --output Dataset.zip\n","# !unzip Dataset.zip"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"v1gYr_aoNDue"},"source":["# Data"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Mz_NpuAipk3h"},"source":["## Dataset\n","- Original dataset is [Voxceleb1](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/).\n","- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb1.\n","- We randomly select 600 speakers from Voxceleb1.\n","- Then preprocess the raw waveforms into mel-spectrograms.\n","\n","- Args:\n","  - data_dir: The path to the data directory.\n","  - metadata_path: The path to the metadata.\n","  - segment_len: The length of audio segment for training. \n","- The architecture of data directory \\\\\n","  - data directory \\\\\n","  |---- metadata.json \\\\\n","  |---- testdata.json \\\\\n","  |---- mapping.json \\\\\n","  |---- uttr-{random string}.pt \\\\\n","\n","- The information in metadata\n","  - \"n_mels\": The dimention of mel-spectrogram.\n","  - \"speakers\": A dictionary. \n","    - Key: speaker ids.\n","    - value: \"feature_path\" and \"mel_len\"\n","\n","\n","For efficiency, we segment the mel-spectrograms into segments in the traing step."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T18:15:33.116465Z","iopub.status.busy":"2023-05-01T18:15:33.116095Z","iopub.status.idle":"2023-05-01T18:15:33.132118Z","shell.execute_reply":"2023-05-01T18:15:33.131006Z","shell.execute_reply.started":"2023-05-01T18:15:33.116427Z"},"id":"cd7hoGhYtbXQ","trusted":true},"outputs":[],"source":["import os\n","import json\n","import torch\n","import random\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n"," \n","class myDataset(Dataset):\n","    def __init__(self, data_dir, segment_len=128):\n","        self.data_dir = data_dir\n","        self.segment_len = segment_len\n","\n","        # Load the mapping from speaker neme to their corresponding id. \n","        mapping_path = Path(data_dir) / \"mapping.json\"\n","        mapping = json.load(mapping_path.open())\n","        self.speaker2id = mapping[\"speaker2id\"]\n","\n","        # Load metadata of training data.\n","        metadata_path = Path(data_dir) / \"metadata.json\"\n","        metadata = json.load(open(metadata_path))[\"speakers\"] #metadata中存放的key是speaker的id，value是speaker的feature、len\n","#         n_mels：在對語音數據進行處理時，從每一个時間维度上選取n_mels个维度来表示這個feature\n","        \n","        # Get the total number of speaker.\n","        self.speaker_num = len(metadata.keys())\n","        self.data = []\n","        for speaker in metadata.keys():#遍歷每一個speaker id\n","            for utterances in metadata[speaker]:#通過speaker id取得speaker所有feature和len\n","                self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        feat_path, speaker = self.data[index]\n","        # Load preprocessed mel-spectrogram.\n","        mel = torch.load(os.path.join(self.data_dir, feat_path))\n","\n","        # Segmemt mel-spectrogram into \"segment_len\" frames.\n","        if len(mel) > self.segment_len:#將feature切成固定長度\n","          # Randomly get the starting point of the segment.\n","          start = random.randint(0, len(mel) - self.segment_len)\n","          # Get a segment with \"segment_len\" frames.\n","          mel = torch.FloatTensor(mel[start:start+self.segment_len])#擷取長度為segment_len的片段，mel.size():torch.size([128, 40])\n","        else:\n","            mel = torch.FloatTensor(mel)\n","        # Turn the speaker id into long for computing loss later.\n","        speaker = torch.FloatTensor([speaker]).long()\n","        return mel, speaker\n","\n","    def get_speaker_number(self):\n","        return self.speaker_num"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mqJxjoi_NGnB"},"source":["## Dataloader\n","- Split dataset into training dataset(90%) and validation dataset(10%).\n","- Create dataloader to iterate the data.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T18:15:33.168769Z","iopub.status.busy":"2023-05-01T18:15:33.168044Z","iopub.status.idle":"2023-05-01T18:15:33.180487Z","shell.execute_reply":"2023-05-01T18:15:33.179295Z","shell.execute_reply.started":"2023-05-01T18:15:33.168728Z"},"id":"zuT1AuFENI8t","trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","from torch.nn.utils.rnn import pad_sequence\n","\n","\n","def collate_batch(batch):\n","    # Process features within a batch.\n","    \"\"\"Collate a batch of data.\"\"\"\n","    mel, speaker = zip(*batch)#將一個batch中的mel和speaker分開，各自單獨形成一個數組\n","    #當mel中元素數量不相同時，將所有的mel元素填充到最長的元素的長度，填充的值由padding_value決定\n","    # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n","    mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n","    # mel: (batch size, length, 40)\n","    return mel, torch.FloatTensor(speaker).long()\n","\n","\n","def get_dataloader(data_dir, batch_size, n_workers):\n","    \"\"\"Generate dataloader\"\"\"\n","    dataset = myDataset(data_dir)\n","    speaker_num = dataset.get_speaker_number()\n","    # Split dataset into training dataset and validation dataset\n","    trainlen = int(0.9 * len(dataset))\n","    lengths = [trainlen, len(dataset) - trainlen]\n","    trainset, validset = random_split(dataset, lengths)# random_split(data全部物件，[分割比例])\n","\n","    train_loader = DataLoader(\n","        trainset,\n","        batch_size = batch_size,\n","        shuffle = True,\n","        drop_last = True,\n","        num_workers = n_workers,\n","        pin_memory = True,\n","        collate_fn = collate_batch,\n","    )\n","    valid_loader = DataLoader(\n","        validset,\n","        batch_size = batch_size,\n","        num_workers = n_workers,\n","        drop_last = True,\n","        pin_memory = True,\n","        collate_fn = collate_batch,\n","    )\n","\n","    return train_loader, valid_loader, speaker_num\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"X0x6eXiHpr4R"},"source":["# Model\n","- TransformerEncoderLayer:\n","  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n","  - Parameters:\n","    - d_model: the number of expected features of the input (required).\n","\n","    - nhead: the number of heads of the multiheadattention models (required).\n","\n","    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n","\n","    - dropout: the dropout value (default=0.1).\n","\n","    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n","\n","- TransformerEncoder:\n","  - TransformerEncoder is a stack of N transformer encoder layers\n","  - Parameters:\n","    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n","\n","    - num_layers: the number of sub-encoder-layers in the encoder (required).\n","\n","    - norm: the layer normalization component (optional)."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T18:15:33.183450Z","iopub.status.busy":"2023-05-01T18:15:33.183065Z","iopub.status.idle":"2023-05-01T18:15:33.194840Z","shell.execute_reply":"2023-05-01T18:15:33.193712Z","shell.execute_reply.started":"2023-05-01T18:15:33.183413Z"},"id":"SHX4eVj4tjtd","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from conformer import ConformerBlock \n","\n","\n","class Classifier(nn.Module):\n","    def __init__(self, d_model = 512, n_spks = 600, dropout = 0.1):#d_model:輸入的feature個數\n","        super().__init__()\n","        # Project the dimension of features from that of input into d_model.\n","        self.prenet = nn.Linear(40, d_model)\n","        # TODO:\n","        self.conformer_block = ConformerBlock(\n","            dim = d_model,\n","            dim_head = 4,\n","            heads = 4,\n","            ff_mult = 4,\n","            conv_expansion_factor = 2,\n","            conv_kernel_size = 20,\n","            attn_dropout = dropout,\n","            ff_dropout = dropout,\n","            conv_dropout = dropout\n","        )\n","        # self.conformer_block = ConformerBlock(\n","        #     dim = d_model,\n","        #     dim_head = 64,\n","        #     heads = 8,\n","        #     ff_mult = 4,\n","        #     conv_expansion_factor = 2,\n","        #     conv_kernel_size = 31,\n","        #     attn_dropout = dropout,\n","        #     ff_dropout = dropout,\n","        #     conv_dropout = dropout\n","        # )\n","        self.encoder_layer = nn.TransformerEncoderLayer(\n","            d_model = d_model,\n","            dim_feedforward = 256,\n","            # nhead = 2 \n","            nhead = 1 \n","        )\n","        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n","\n","        # Project the the dimension of features from d_model into speaker nums.\n","        self.pred_layer = nn.Sequential(\n","            # nn.Linear(d_model, d_model),\n","            # nn.ReLU(),\n","            nn.BatchNorm1d(d_model),\n","            nn.Linear(d_model, n_spks),\n","        )\n","\n","    def forward(self, mels):\n","        \"\"\"\n","        args:\n","          mels: (batch size, length, 40)\n","        return:\n","          out: (batch size, n_spks)\n","        \"\"\"\n","        # out: (batch size, length, d_model)\n","        out = self.prenet(mels)\n","        # out: (length, batch size, d_model)\n","        out = out.permute(1, 0, 2)#交換dim = 0和dim = 1\n","        # The encoder layer expect features in the shape of (length, batch size, d_model).\n","        # out = self.encoder_layer(out)\n","#         out = self.encoder(out)\n","        out = self.conformer_block(out)\n","        # out: (batch size, length, d_model)\n","        out = out.transpose(0, 1)\n","        # mean pooling\n","        stats = out.mean(dim=1)#求平均並去除dim = 1\n","\n","        # out: (batch, n_spks)\n","        out = self.pred_layer(stats)\n","        return out\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-__DolPGpvDZ"},"source":["# Learning rate schedule\n","- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n","- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n","- The warmup schedule\n","  - Set learning rate to 0 in the beginning.\n","  - The learning rate increases linearly from 0 to initial learning rate during warmup period."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T18:15:33.284704Z","iopub.status.busy":"2023-05-01T18:15:33.284346Z","iopub.status.idle":"2023-05-01T18:15:33.294436Z","shell.execute_reply":"2023-05-01T18:15:33.293027Z","shell.execute_reply.started":"2023-05-01T18:15:33.284652Z"},"id":"K-0816BntqT9","trusted":true},"outputs":[],"source":["import math\n","\n","import torch\n","from torch.optim import Optimizer\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","\n","def get_cosine_schedule_with_warmup(\n","    optimizer: Optimizer,\n","    num_warmup_steps: int,\n","    num_training_steps: int,\n","    num_cycles: float = 0.5,\n","    last_epoch: int = -1,\n","):\n","    \"\"\"\n","    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n","    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n","    initial lr set in the optimizer.\n","\n","    Args:\n","    optimizer (:class:`~torch.optim.Optimizer`):\n","      The optimizer for which to schedule the learning rate.\n","    num_warmup_steps (:obj:`int`):\n","      The number of steps for the warmup phase.\n","    num_training_steps (:obj:`int`):\n","      The total number of training steps.\n","    num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n","      The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n","      following a half-cosine).\n","    last_epoch (:obj:`int`, `optional`, defaults to -1):\n","      The index of the last epoch when resuming training.\n","\n","    Return:\n","    :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n","    \"\"\"\n","\n","    def lr_lambda(current_step):\n","        # Warmup\n","        if current_step < num_warmup_steps:\n","            return float(current_step) / float(max(1, num_warmup_steps))\n","        # decadence\n","        progress = float(current_step - num_warmup_steps) / float(\n","            max(1, num_training_steps - num_warmup_steps)\n","        )\n","        return max(\n","            0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n","        )\n","\n","    return LambdaLR(optimizer, lr_lambda, last_epoch)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IP03FFo9K8DS"},"source":["# Model Function\n","- Model forward function."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T18:15:33.297720Z","iopub.status.busy":"2023-05-01T18:15:33.296980Z","iopub.status.idle":"2023-05-01T18:15:33.305716Z","shell.execute_reply":"2023-05-01T18:15:33.304761Z","shell.execute_reply.started":"2023-05-01T18:15:33.297669Z"},"id":"fohaLEFJK9-t","trusted":true},"outputs":[],"source":["import torch\n","\n","#每遍歷一個batch都會呼叫一次model_fn\n","def model_fn(batch, model, criterion, device):\n","    \"\"\"Forward a batch through the model.\"\"\"\n","\n","    mels, labels = batch\n","    mels = mels.to(device)\n","    labels = labels.to(device)\n","\n","    outs = model(mels)\n","\n","    loss = criterion(outs, labels)\n","\n","    # Get the speaker id with highest probability.\n","    preds = outs.argmax(1)\n","    # Compute accuracy.\n","    accuracy = torch.mean((preds == labels).float())\n","\n","    return loss, accuracy\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"F7cg-YrzLQcf"},"source":["# Validate\n","- Calculate accuracy of the validation set."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T18:15:33.307740Z","iopub.status.busy":"2023-05-01T18:15:33.307334Z","iopub.status.idle":"2023-05-01T18:15:33.319357Z","shell.execute_reply":"2023-05-01T18:15:33.318368Z","shell.execute_reply.started":"2023-05-01T18:15:33.307693Z"},"id":"mD-_p6nWLO2L","trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","\n","\n","def valid(dataloader, model, criterion, device): \n","    \"\"\"Validate on validation set.\"\"\"\n","\n","    model.eval()\n","    running_loss = 0.0\n","    running_accuracy = 0.0\n","    pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n","\n","    for i, batch in enumerate(dataloader):\n","        with torch.no_grad():\n","            loss, accuracy = model_fn(batch, model, criterion, device)\n","            running_loss += loss.item()\n","            running_accuracy += accuracy.item()\n","\n","        pbar.update(dataloader.batch_size)\n","        pbar.set_postfix(\n","            loss=f\"{running_loss / (i+1):.2f}\",\n","            accuracy=f\"{running_accuracy / (i+1):.2f}\",\n","        )\n","\n","    pbar.close()\n","    model.train()\n","\n","    return running_accuracy / len(dataloader)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"noHXyal5p1W5"},"source":["# Main function"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T18:15:33.321661Z","iopub.status.busy":"2023-05-01T18:15:33.321196Z","iopub.status.idle":"2023-05-01T19:39:09.231293Z","shell.execute_reply":"2023-05-01T19:39:09.228301Z","shell.execute_reply.started":"2023-05-01T18:15:33.321625Z"},"id":"chRQE7oYtw62","outputId":"83ddc97a-05df-474b-c972-15b92823b983","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[Info]: Use cuda now!\n","[Info]: loading data!\n","[Info]: Finish loading data!\n","[Info]: Finish creating model!\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:48<00:00, 11.90 step/s, accuracy=0.31, loss=3.39, step=2000]\n","Valid: 100% 6336/6344 [00:10<00:00, 602.84 uttr/s, accuracy=0.32, loss=3.20]\n","Train: 100% 2000/2000 [02:45<00:00, 12.09 step/s, accuracy=0.44, loss=2.30, step=4000]\n","Valid: 100% 6336/6344 [00:09<00:00, 649.50 uttr/s, accuracy=0.49, loss=2.34]\n","Train: 100% 2000/2000 [02:46<00:00, 11.99 step/s, accuracy=0.59, loss=1.55, step=6000]\n","Valid: 100% 6336/6344 [00:10<00:00, 594.66 uttr/s, accuracy=0.58, loss=1.88]\n","Train: 100% 2000/2000 [02:46<00:00, 11.99 step/s, accuracy=0.84, loss=0.95, step=8000]\n","Valid: 100% 6336/6344 [00:09<00:00, 639.15 uttr/s, accuracy=0.63, loss=1.60]\n","Train: 100% 2000/2000 [02:45<00:00, 12.07 step/s, accuracy=0.75, loss=1.44, step=1e+4]\n","Valid: 100% 6336/6344 [00:10<00:00, 598.75 uttr/s, accuracy=0.67, loss=1.45]\n","Train:   0% 2/2000 [00:00<04:39,  7.15 step/s, accuracy=0.69, loss=1.19, step=1e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 10000, best model saved. (accuracy=0.6728)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:45<00:00, 12.05 step/s, accuracy=0.78, loss=0.80, step=12000]\n","Valid: 100% 6336/6344 [00:10<00:00, 633.19 uttr/s, accuracy=0.69, loss=1.36]\n","Train: 100% 2000/2000 [02:46<00:00, 12.04 step/s, accuracy=0.75, loss=0.92, step=14000]\n","Valid: 100% 6336/6344 [00:10<00:00, 626.63 uttr/s, accuracy=0.72, loss=1.23]\n","Train: 100% 2000/2000 [02:46<00:00, 12.01 step/s, accuracy=0.75, loss=0.75, step=16000]\n","Valid: 100% 6336/6344 [00:09<00:00, 651.98 uttr/s, accuracy=0.72, loss=1.24]\n","Train: 100% 2000/2000 [02:45<00:00, 12.10 step/s, accuracy=0.84, loss=0.62, step=18000]\n","Valid: 100% 6336/6344 [00:10<00:00, 627.82 uttr/s, accuracy=0.73, loss=1.17]\n","Train: 100% 2000/2000 [02:44<00:00, 12.17 step/s, accuracy=0.88, loss=0.53, step=2e+4] \n","Valid: 100% 6336/6344 [00:09<00:00, 652.13 uttr/s, accuracy=0.76, loss=1.07]\n","Train:   0% 2/2000 [00:00<03:55,  8.47 step/s, accuracy=0.84, loss=0.54, step=2e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 20000, best model saved. (accuracy=0.7590)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:43<00:00, 12.21 step/s, accuracy=0.88, loss=0.44, step=22000]\n","Valid: 100% 6336/6344 [00:10<00:00, 632.48 uttr/s, accuracy=0.76, loss=1.06]\n","Train: 100% 2000/2000 [02:44<00:00, 12.14 step/s, accuracy=0.88, loss=0.39, step=24000]\n","Valid: 100% 6336/6344 [00:09<00:00, 659.48 uttr/s, accuracy=0.77, loss=1.02]\n","Train: 100% 2000/2000 [02:44<00:00, 12.15 step/s, accuracy=0.91, loss=0.29, step=26000]\n","Valid: 100% 6336/6344 [00:09<00:00, 641.26 uttr/s, accuracy=0.78, loss=0.98]\n","Train: 100% 2000/2000 [02:43<00:00, 12.23 step/s, accuracy=0.84, loss=0.53, step=28000]\n","Valid: 100% 6336/6344 [00:09<00:00, 667.29 uttr/s, accuracy=0.79, loss=0.95]\n","Train: 100% 2000/2000 [02:43<00:00, 12.23 step/s, accuracy=0.91, loss=0.26, step=3e+4] \n","Valid: 100% 6336/6344 [00:13<00:00, 453.11 uttr/s, accuracy=0.78, loss=0.97]\n","Train:   0% 1/2000 [00:00<10:59,  3.03 step/s, accuracy=0.88, loss=0.51, step=3e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 30000, best model saved. (accuracy=0.7876)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:18<00:00, 10.07 step/s, accuracy=0.84, loss=0.73, step=32000]\n","Valid: 100% 6336/6344 [00:16<00:00, 374.52 uttr/s, accuracy=0.80, loss=0.94]\n","Train: 100% 2000/2000 [03:05<00:00, 10.79 step/s, accuracy=0.88, loss=0.36, step=34000]\n","Valid: 100% 6336/6344 [00:09<00:00, 664.01 uttr/s, accuracy=0.81, loss=0.89]\n","Train: 100% 2000/2000 [02:43<00:00, 12.21 step/s, accuracy=0.91, loss=0.46, step=36000]\n","Valid: 100% 6336/6344 [00:09<00:00, 646.38 uttr/s, accuracy=0.81, loss=0.90]\n","Train: 100% 2000/2000 [02:44<00:00, 12.15 step/s, accuracy=0.97, loss=0.18, step=38000]\n","Valid: 100% 6336/6344 [00:09<00:00, 648.20 uttr/s, accuracy=0.81, loss=0.92]\n","Train: 100% 2000/2000 [02:44<00:00, 12.15 step/s, accuracy=0.88, loss=0.49, step=4e+4] \n","Valid: 100% 6336/6344 [00:10<00:00, 631.64 uttr/s, accuracy=0.81, loss=0.88]\n","Train:   0% 2/2000 [00:00<04:21,  7.65 step/s, accuracy=0.97, loss=0.34, step=4e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 40000, best model saved. (accuracy=0.8147)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:43<00:00, 12.25 step/s, accuracy=0.88, loss=0.36, step=42000]\n","Valid: 100% 6336/6344 [00:09<00:00, 653.92 uttr/s, accuracy=0.82, loss=0.85]\n","Train: 100% 2000/2000 [02:42<00:00, 12.33 step/s, accuracy=0.88, loss=0.60, step=44000]\n","Valid: 100% 6336/6344 [00:06<00:00, 942.99 uttr/s, accuracy=0.82, loss=0.89]\n","Train: 100% 2000/2000 [02:43<00:00, 12.25 step/s, accuracy=0.94, loss=0.34, step=46000]\n","Valid: 100% 6336/6344 [00:06<00:00, 980.52 uttr/s, accuracy=0.83, loss=0.85]\n","Train: 100% 2000/2000 [02:43<00:00, 12.25 step/s, accuracy=0.97, loss=0.07, step=48000]\n","Valid: 100% 6336/6344 [00:06<00:00, 970.95 uttr/s, accuracy=0.83, loss=0.85]\n","Train: 100% 2000/2000 [02:43<00:00, 12.24 step/s, accuracy=0.97, loss=0.21, step=5e+4] \n","Valid: 100% 6336/6344 [00:10<00:00, 632.69 uttr/s, accuracy=0.83, loss=0.83]\n","Train:   0% 3/2000 [00:00<02:56, 11.32 step/s, accuracy=0.91, loss=0.34, step=5e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 50000, best model saved. (accuracy=0.8321)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:42<00:00, 12.30 step/s, accuracy=0.94, loss=0.34, step=52000]\n","Valid: 100% 6336/6344 [00:09<00:00, 667.26 uttr/s, accuracy=0.84, loss=0.80]\n","Train: 100% 2000/2000 [02:42<00:00, 12.31 step/s, accuracy=0.94, loss=0.22, step=54000]\n","Valid: 100% 6336/6344 [00:06<00:00, 979.47 uttr/s, accuracy=0.85, loss=0.74] \n","Train: 100% 2000/2000 [02:43<00:00, 12.25 step/s, accuracy=0.91, loss=0.31, step=56000]\n","Valid: 100% 6336/6344 [00:06<00:00, 982.75 uttr/s, accuracy=0.84, loss=0.78] \n","Train: 100% 2000/2000 [02:43<00:00, 12.26 step/s, accuracy=0.97, loss=0.27, step=58000]\n","Valid: 100% 6336/6344 [00:06<00:00, 987.91 uttr/s, accuracy=0.83, loss=0.81] \n","Train: 100% 2000/2000 [02:43<00:00, 12.27 step/s, accuracy=0.91, loss=0.29, step=6e+4] \n","Valid: 100% 6336/6344 [00:06<00:00, 980.99 uttr/s, accuracy=0.84, loss=0.82] \n","Train:   0% 2/2000 [00:00<03:50,  8.67 step/s, accuracy=1.00, loss=0.03, step=6e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 60000, best model saved. (accuracy=0.8452)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:42<00:00, 12.32 step/s, accuracy=0.94, loss=0.13, step=62000]\n","Valid: 100% 6336/6344 [00:09<00:00, 661.67 uttr/s, accuracy=0.85, loss=0.76]\n","Train: 100% 2000/2000 [02:43<00:00, 12.27 step/s, accuracy=0.88, loss=0.26, step=64000]\n","Valid: 100% 6336/6344 [00:06<00:00, 979.41 uttr/s, accuracy=0.85, loss=0.73]\n","Train: 100% 2000/2000 [02:43<00:00, 12.26 step/s, accuracy=0.97, loss=0.18, step=66000]\n","Valid: 100% 6336/6344 [00:09<00:00, 633.64 uttr/s, accuracy=0.85, loss=0.77]\n","Train: 100% 2000/2000 [02:43<00:00, 12.26 step/s, accuracy=1.00, loss=0.05, step=68000]\n","Valid: 100% 6336/6344 [00:09<00:00, 665.58 uttr/s, accuracy=0.85, loss=0.74]\n","Train: 100% 2000/2000 [02:42<00:00, 12.28 step/s, accuracy=1.00, loss=0.05, step=7e+4] \n","Valid: 100% 6336/6344 [00:06<00:00, 981.93 uttr/s, accuracy=0.86, loss=0.70] \n","Train:   0% 2/2000 [00:00<03:31,  9.45 step/s, accuracy=1.00, loss=0.04, step=7e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 70000, best model saved. (accuracy=0.8638)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:42<00:00, 12.29 step/s, accuracy=0.97, loss=0.06, step=72000]\n","Valid: 100% 6336/6344 [00:06<00:00, 933.44 uttr/s, accuracy=0.86, loss=0.67] \n","Train: 100% 2000/2000 [02:43<00:00, 12.24 step/s, accuracy=1.00, loss=0.04, step=74000]\n","Valid: 100% 6336/6344 [00:06<00:00, 982.98 uttr/s, accuracy=0.86, loss=0.70] \n","Train: 100% 2000/2000 [02:43<00:00, 12.24 step/s, accuracy=0.97, loss=0.07, step=76000]\n","Valid: 100% 6336/6344 [00:06<00:00, 983.64 uttr/s, accuracy=0.86, loss=0.74]\n","Train: 100% 2000/2000 [02:43<00:00, 12.26 step/s, accuracy=0.97, loss=0.05, step=78000]\n","Valid: 100% 6336/6344 [00:06<00:00, 989.18 uttr/s, accuracy=0.86, loss=0.67] \n","Train: 100% 2000/2000 [02:42<00:00, 12.32 step/s, accuracy=1.00, loss=0.06, step=8e+4] \n","Valid: 100% 6336/6344 [00:06<00:00, 988.94 uttr/s, accuracy=0.87, loss=0.67] \n","Train:   0% 2/2000 [00:00<04:29,  7.43 step/s, accuracy=0.97, loss=0.16, step=8e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 80000, best model saved. (accuracy=0.8688)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:43<00:00, 12.27 step/s, accuracy=1.00, loss=0.06, step=82000]\n","Valid: 100% 6336/6344 [00:09<00:00, 644.33 uttr/s, accuracy=0.87, loss=0.68]\n","Train: 100% 2000/2000 [02:44<00:00, 12.17 step/s, accuracy=0.94, loss=0.14, step=84000]\n","Valid: 100% 6336/6344 [00:09<00:00, 657.97 uttr/s, accuracy=0.88, loss=0.64]\n","Train: 100% 2000/2000 [02:44<00:00, 12.17 step/s, accuracy=1.00, loss=0.02, step=86000]\n","Valid: 100% 6336/6344 [00:09<00:00, 661.30 uttr/s, accuracy=0.87, loss=0.63]\n","Train: 100% 2000/2000 [02:44<00:00, 12.17 step/s, accuracy=1.00, loss=0.06, step=88000]\n","Valid: 100% 6336/6344 [00:06<00:00, 938.50 uttr/s, accuracy=0.88, loss=0.63]\n","Train: 100% 2000/2000 [02:43<00:00, 12.22 step/s, accuracy=1.00, loss=0.02, step=9e+4] \n","Valid: 100% 6336/6344 [00:06<00:00, 973.43 uttr/s, accuracy=0.88, loss=0.64]\n","Train:   0% 3/2000 [00:00<02:59, 11.11 step/s, accuracy=1.00, loss=0.01, step=9e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 90000, best model saved. (accuracy=0.8788)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:44<00:00, 12.15 step/s, accuracy=0.94, loss=0.11, step=92000]\n","Valid: 100% 6336/6344 [00:06<00:00, 968.18 uttr/s, accuracy=0.88, loss=0.62]\n","Train: 100% 2000/2000 [02:44<00:00, 12.15 step/s, accuracy=1.00, loss=0.04, step=94000]\n","Valid: 100% 6336/6344 [00:06<00:00, 966.14 uttr/s, accuracy=0.88, loss=0.61]\n","Train: 100% 2000/2000 [02:44<00:00, 12.17 step/s, accuracy=0.97, loss=0.11, step=96000]\n","Valid: 100% 6336/6344 [00:06<00:00, 964.58 uttr/s, accuracy=0.88, loss=0.61]\n","Train: 100% 2000/2000 [02:43<00:00, 12.22 step/s, accuracy=1.00, loss=0.03, step=98000]\n","Valid: 100% 6336/6344 [00:06<00:00, 967.57 uttr/s, accuracy=0.88, loss=0.59]\n","Train: 100% 2000/2000 [02:44<00:00, 12.19 step/s, accuracy=0.97, loss=0.07, step=1e+5] \n","Valid: 100% 6336/6344 [00:08<00:00, 740.78 uttr/s, accuracy=0.88, loss=0.60]\n","Train:   0% 2/2000 [00:00<03:38,  9.14 step/s, accuracy=1.00, loss=0.01, step=1e+5]"]},{"name":"stdout","output_type":"stream","text":["Step 100000, best model saved. (accuracy=0.8846)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:43<00:00, 12.26 step/s, accuracy=1.00, loss=0.00, step=102000]\n","Valid: 100% 6336/6344 [00:06<00:00, 964.28 uttr/s, accuracy=0.89, loss=0.57] \n","Train: 100% 2000/2000 [02:43<00:00, 12.27 step/s, accuracy=1.00, loss=0.02, step=104000]\n","Valid: 100% 6336/6344 [00:06<00:00, 987.90 uttr/s, accuracy=0.89, loss=0.59] \n","Train: 100% 2000/2000 [02:43<00:00, 12.26 step/s, accuracy=1.00, loss=0.03, step=106000]\n","Valid: 100% 6336/6344 [00:09<00:00, 639.18 uttr/s, accuracy=0.89, loss=0.57]\n","Train: 100% 2000/2000 [02:41<00:00, 12.35 step/s, accuracy=1.00, loss=0.01, step=108000]\n","Valid: 100% 6336/6344 [00:06<00:00, 978.13 uttr/s, accuracy=0.89, loss=0.58]\n","Train: 100% 2000/2000 [02:43<00:00, 12.23 step/s, accuracy=0.97, loss=0.07, step=110000]\n","Valid: 100% 6336/6344 [00:06<00:00, 970.35 uttr/s, accuracy=0.90, loss=0.54]\n","Train:   0% 3/2000 [00:00<03:01, 11.01 step/s, accuracy=1.00, loss=0.01, step=110002]"]},{"name":"stdout","output_type":"stream","text":["Step 110000, best model saved. (accuracy=0.8977)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:44<00:00, 12.16 step/s, accuracy=1.00, loss=0.01, step=112000]\n","Valid: 100% 6336/6344 [00:06<00:00, 961.64 uttr/s, accuracy=0.90, loss=0.51]\n","Train: 100% 2000/2000 [02:44<00:00, 12.17 step/s, accuracy=1.00, loss=0.01, step=114000]\n","Valid: 100% 6336/6344 [00:06<00:00, 965.23 uttr/s, accuracy=0.90, loss=0.55]\n","Train: 100% 2000/2000 [02:43<00:00, 12.20 step/s, accuracy=1.00, loss=0.00, step=116000]\n","Valid: 100% 6336/6344 [00:10<00:00, 626.85 uttr/s, accuracy=0.90, loss=0.55]\n","Train: 100% 2000/2000 [02:44<00:00, 12.19 step/s, accuracy=1.00, loss=0.02, step=118000]\n","Valid: 100% 6336/6344 [00:09<00:00, 675.80 uttr/s, accuracy=0.90, loss=0.53]\n","Train: 100% 2000/2000 [02:44<00:00, 12.18 step/s, accuracy=0.97, loss=0.09, step=120000]\n","Valid: 100% 6336/6344 [00:06<00:00, 969.05 uttr/s, accuracy=0.90, loss=0.53]\n","Train:   0% 2/2000 [00:00<03:37,  9.20 step/s, accuracy=0.97, loss=0.06, step=120002]"]},{"name":"stdout","output_type":"stream","text":["Step 120000, best model saved. (accuracy=0.9045)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:43<00:00, 12.27 step/s, accuracy=1.00, loss=0.03, step=122000]\n","Valid: 100% 6336/6344 [00:06<00:00, 981.18 uttr/s, accuracy=0.90, loss=0.54] \n","Train: 100% 2000/2000 [02:42<00:00, 12.27 step/s, accuracy=1.00, loss=0.01, step=124000]\n","Valid: 100% 6336/6344 [00:06<00:00, 971.73 uttr/s, accuracy=0.90, loss=0.54]\n","Train: 100% 2000/2000 [02:41<00:00, 12.36 step/s, accuracy=1.00, loss=0.01, step=126000]\n","Valid: 100% 6336/6344 [00:06<00:00, 977.41 uttr/s, accuracy=0.91, loss=0.50] \n","Train: 100% 2000/2000 [02:42<00:00, 12.27 step/s, accuracy=1.00, loss=0.00, step=128000]\n","Valid: 100% 6336/6344 [00:06<00:00, 977.12 uttr/s, accuracy=0.91, loss=0.52] \n","Train: 100% 2000/2000 [02:43<00:00, 12.27 step/s, accuracy=1.00, loss=0.00, step=130000]\n","Valid: 100% 6336/6344 [00:06<00:00, 973.57 uttr/s, accuracy=0.91, loss=0.52]\n","Train:   0% 2/2000 [00:00<03:33,  9.38 step/s, accuracy=1.00, loss=0.00, step=130002]"]},{"name":"stdout","output_type":"stream","text":["Step 130000, best model saved. (accuracy=0.9062)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:43<00:00, 12.27 step/s, accuracy=1.00, loss=0.02, step=132000]\n","Valid: 100% 6336/6344 [00:09<00:00, 672.58 uttr/s, accuracy=0.90, loss=0.50]\n","Train: 100% 2000/2000 [02:42<00:00, 12.30 step/s, accuracy=1.00, loss=0.00, step=134000]\n","Valid: 100% 6336/6344 [00:09<00:00, 655.93 uttr/s, accuracy=0.90, loss=0.52]\n","Train: 100% 2000/2000 [02:42<00:00, 12.33 step/s, accuracy=1.00, loss=0.00, step=136000]\n","Valid: 100% 6336/6344 [00:06<00:00, 979.05 uttr/s, accuracy=0.90, loss=0.51]\n","Train: 100% 2000/2000 [02:42<00:00, 12.27 step/s, accuracy=1.00, loss=0.01, step=138000]\n","Valid: 100% 6336/6344 [00:06<00:00, 974.84 uttr/s, accuracy=0.90, loss=0.54]\n","Train: 100% 2000/2000 [02:43<00:00, 12.27 step/s, accuracy=1.00, loss=0.00, step=140000]\n","Valid: 100% 6336/6344 [00:06<00:00, 972.28 uttr/s, accuracy=0.91, loss=0.48]\n","Train:   0% 3/2000 [00:00<02:56, 11.31 step/s, accuracy=1.00, loss=0.01, step=140002]"]},{"name":"stdout","output_type":"stream","text":["Step 140000, best model saved. (accuracy=0.9078)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [02:42<00:00, 12.28 step/s, accuracy=1.00, loss=0.00, step=142000]\n","Valid: 100% 6336/6344 [00:06<00:00, 977.48 uttr/s, accuracy=0.91, loss=0.48] \n","Train: 100% 2000/2000 [02:41<00:00, 12.35 step/s, accuracy=1.00, loss=0.00, step=144000]\n","Valid: 100% 6336/6344 [00:06<00:00, 982.07 uttr/s, accuracy=0.91, loss=0.50] \n","Train: 100% 2000/2000 [02:42<00:00, 12.28 step/s, accuracy=1.00, loss=0.00, step=146000]\n","Valid: 100% 6336/6344 [00:06<00:00, 975.75 uttr/s, accuracy=0.91, loss=0.50]\n","Train: 100% 2000/2000 [02:42<00:00, 12.28 step/s, accuracy=1.00, loss=0.01, step=148000]\n","Valid: 100% 6336/6344 [00:09<00:00, 649.36 uttr/s, accuracy=0.91, loss=0.52]\n","Train: 100% 2000/2000 [02:43<00:00, 12.21 step/s, accuracy=1.00, loss=0.00, step=150000]\n","Valid: 100% 6336/6344 [00:09<00:00, 643.79 uttr/s, accuracy=0.90, loss=0.50]\n","Train:   0% 0/2000 [00:00<?, ? step/s]"]},{"name":"stdout","output_type":"stream","text":["Step 150000, best model saved. (accuracy=0.9108)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, random_split\n","\n","#跑vaild_steps個batch跑一遍驗證集\n","def parse_args():\n","    \"\"\"arguments\"\"\"\n","    config = {\n","        \"data_dir\": \"./Dataset\",\n","        \"save_path\": \"model.ckpt\",\n","        \"batch_size\": 32,\n","        # \"batch_size\": 128,\n","        \"n_workers\": 0,\n","        \"valid_steps\": 2000,\n","        \"warmup_steps\": 1000, #從0上升到預設lr需要的step\n","        \"save_steps\": 10000,\n","        \"total_steps\": 150000,#訓練過程的總step(一共進行total_steps輪)\n","    }\n","\n","    return config\n","\n","\n","def main(\n","    data_dir,\n","    save_path,\n","    batch_size,\n","    n_workers,\n","    valid_steps,\n","    warmup_steps,\n","    total_steps,\n","    save_steps,\n","):\n","    \"\"\"Main function.\"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"[Info]: Use {device} now!\")\n","\n","    train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n","    print(f\"[Info]: loading data!\")\n","    train_iterator = iter(train_loader)\n","    print(f\"[Info]: Finish loading data!\",flush = True)\n","\n","    model = Classifier(n_spks=speaker_num).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = AdamW(model.parameters(), lr=1e-3)\n","    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","    print(f\"[Info]: Finish creating model!\",flush = True)\n","\n","    best_accuracy = -1.0\n","    best_state_dict = None\n","\n","    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n","\n","    for step in range(total_steps):\n","        # Get data\n","        try:\n","            batch = next(train_iterator)#next返回iter，即下一個batch\n","        except StopIteration:\n","            train_iterator = iter(train_loader)\n","            batch = next(train_iterator)\n","\n","        loss, accuracy = model_fn(batch, model, criterion, device)\n","        batch_loss = loss.item()\n","        batch_accuracy = accuracy.item()\n","\n","        # Updata model\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        # Log\n","        pbar.update()\n","        pbar.set_postfix(\n","          loss = f\"{batch_loss:.2f}\",\n","          accuracy = f\"{batch_accuracy:.2f}\",\n","          step = step + 1,\n","        )\n","\n","        # Do validation\n","        if (step + 1) % valid_steps == 0:\n","            pbar.close()\n","\n","            valid_accuracy = valid(valid_loader, model, criterion, device)\n","\n","            # keep the best model\n","            if valid_accuracy > best_accuracy:\n","                best_accuracy = valid_accuracy\n","                best_state_dict = model.state_dict()\n","\n","            pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n","\n","        # Save the best model so far.\n","        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n","            torch.save(best_state_dict, save_path)\n","            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n","\n","    pbar.close()\n","\n","\n","if __name__ == \"__main__\":\n","    main(**parse_args())\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0R2rx3AyHpQ-"},"source":["# Inference"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pSuI3WY9Fz78"},"source":["## Dataset of inference"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T19:39:09.234737Z","iopub.status.busy":"2023-05-01T19:39:09.234379Z","iopub.status.idle":"2023-05-01T19:39:09.242706Z","shell.execute_reply":"2023-05-01T19:39:09.241549Z","shell.execute_reply.started":"2023-05-01T19:39:09.234705Z"},"id":"4evns0055Dsx","trusted":true},"outputs":[],"source":["import os\n","import json\n","import torch\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","\n","\n","class InferenceDataset(Dataset):\n","    def __init__(self, data_dir):\n","        testdata_path = Path(data_dir) / \"testdata.json\"\n","        metadata = json.load(testdata_path.open())\n","        self.data_dir = data_dir\n","        self.data = metadata[\"utterances\"]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        utterance = self.data[index]\n","        feat_path = utterance[\"feature_path\"]\n","        mel = torch.load(os.path.join(self.data_dir, feat_path))\n","\n","        return feat_path, mel\n","\n","\n","def inference_collate_batch(batch):\n","    \"\"\"Collate a batch of data.\"\"\"\n","    feat_paths, mels = zip(*batch)\n","\n","    return feat_paths, torch.stack(mels)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"oAinHBG1GIWv"},"source":["## Main funcrion of Inference"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T19:39:09.245343Z","iopub.status.busy":"2023-05-01T19:39:09.244854Z","iopub.status.idle":"2023-05-01T19:40:14.631230Z","shell.execute_reply":"2023-05-01T19:40:14.630172Z","shell.execute_reply.started":"2023-05-01T19:39:09.245299Z"},"id":"yQaTt7VDHoRI","outputId":"99b403b7-531a-4094-c3f3-fd99a0dd8060","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[Info]: Use cuda now!\n","[Info]: Finish loading data!\n","[Info]: Finish creating model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bd2118427c04abf92cadaff659042be","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import json\n","import csv\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","def parse_args():\n","    \"\"\"arguments\"\"\"\n","    config = {\n","        \"data_dir\": \"./Dataset\",\n","        \"model_path\": \"./model.ckpt\",\n","        \"output_path\": \"./output4.csv\",\n","    }\n","\n","    return config\n","\n","\n","def main(\n","    data_dir,\n","    model_path,\n","    output_path,\n","):\n","    \"\"\"Main function.\"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"[Info]: Use {device} now!\")\n","\n","    mapping_path = Path(data_dir) / \"mapping.json\"\n","    mapping = json.load(mapping_path.open())\n","\n","    dataset = InferenceDataset(data_dir)\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size = 1,\n","        shuffle = False,\n","        drop_last = False,\n","        num_workers = 0,\n","        collate_fn=inference_collate_batch,\n","    )\n","    print(f\"[Info]: Finish loading data!\",flush = True)\n","\n","    speaker_num = len(mapping[\"id2speaker\"])\n","    model = Classifier(n_spks=speaker_num).to(device)\n","    model.load_state_dict(torch.load(model_path))\n","    model.eval()\n","    print(f\"[Info]: Finish creating model!\",flush = True)\n","\n","    results = [[\"Id\", \"Category\"]]\n","    for feat_paths, mels in tqdm(dataloader):\n","        with torch.no_grad():\n","            mels = mels.to(device)\n","            outs = model(mels)\n","            preds = outs.argmax(1).cpu().numpy()\n","            for feat_path, pred in zip(feat_paths, preds):\n","                results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n","  \n","    with open(output_path, 'w', newline='') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerows(results)\n","\n","if __name__ == \"__main__\":\n","    main(**parse_args())\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":4}
